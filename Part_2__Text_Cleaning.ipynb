{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "**In this Python script, we will clean the data.**\n",
    "\n",
    "This means we will *filter* and *transform* the data to include only the elements of interest, and we will return it in a format that makes it easy to analyze later.\n",
    "\n",
    "**Goals**\n",
    "<ol>\n",
    "    <li> Tokenize the text into words </li>\n",
    "    <li> Remove punctuation and uninformative words </li>\n",
    "    <li> Convert all words to lowercase </li>\n",
    "    <li> Read a frequency distribution </li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "\n",
    "# This new notebook is a new virtual environment, so we have to set it up again.\n",
    "\n",
    "# Import packages\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "from scipy import special\n",
    "from nltk import *\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# This helps matlabplot and jupyter cooperate in showing graphics\n",
    "%matplotlib inline\n",
    "\n",
    "# The relative path to our text file.\n",
    "# In other words, where the file is relative to this Jupyter notebook.\n",
    "pathToFile = 'texts/walden.txt'\n",
    "\n",
    "# Open Walden and read it (hence the 'r') into the variable 'file'\n",
    "file = open( pathToFile, 'r')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "Tokenization is the process of breaking down a big text into units of interest, usually words.\n",
    "\n",
    "But, what is a word?  This might seem like a weird question, but consider the example \"black sheep\", defined by the Cambridge English Dictionary below:\n",
    "\n",
    ">*Someone who embarrasses a group or family because the person is different or has gotten into trouble.*\n",
    "\n",
    "Based on this, you might say that the 'sheep' in 'black sheep' is different than the 'sheep' in the phrase 'my pet sheep', and you have the beginnings of a good argument that 'black sheep' is one word, not two.\n",
    "\n",
    "> NLTK (and most text analytics tools) will tokenize \"my black sheep uncle owns a black cat\" as `['my', 'black', 'sheep', 'uncle', 'owns', 'a', 'black', 'cat']`. **We have a hunch that this is wrong, but whether it matters for your analysis is up to you.**\n",
    "\n",
    "For the purposes of today, we will ignore these kinds of issues, but you should keep in mind that NLTK is built to work for most purposes, not necessarilly your purposes. Before you begin any text analysis project or experiment, you must first define exactly what kind of elements you are interested in, so you can identify those in continuous text.\n",
    "\n",
    "Generally, without more information, a tokenizer will usually split a text into tokens based on spaces and punctuation, a strategy that works for most words.\n",
    "\n",
    ">**Run the code chunk below to see how NLTK tokenizes Walden using its default settings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## Read-in and tokenize Walden ##\n",
    "#################################\n",
    "\n",
    "# Read the Walden file into the variable 'text'\n",
    "walden_raw   = file.read()\n",
    "\n",
    "# Tokenize 'text' with the NLTK function word_tokenize() and save the result to the variable 'tokens'\n",
    "tokens = word_tokenize( walden_raw )\n",
    "\n",
    "# Print the result of the tokenization\n",
    "print( tokens )\n",
    "\n",
    "# Close the Walden text file to release it from memory\n",
    "# We don't need the original text anymore.\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assessing the tokenization\n",
    "\n",
    "Take a minute to look through the tokens that NLTK found. Is there anything there that might get in the way of an analysis of the text?\n",
    "\n",
    "\n",
    "## Filtering non-alphabetical characters\n",
    "\n",
    "Filtering in NLTK usually entails comparing elements between lists. For example, if we want to see whether a particular token (usually equivalent to the idea of word) includes characters outside of the alphabet, we would compare each character to a list of all the letters in the alphabet (uppercase and lowercase).\n",
    "\n",
    "This is the function of NLTK's `isalpha()`.\n",
    "\n",
    ">**In the next code chunk we find the 100 most frequent tokens that include characters outside of the alphabet.** This will help us determine whether we should filter non-alphabetical characters. Follow along with the comments in the code to learn more about each step of the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Find tokens with characters that are not A-Z ##\n",
    "##################################################\n",
    "\n",
    "# Compare each token in Walden to the alphabet, returning when the token includes non-alphabetical characters\n",
    "notalpha = [ token for token in tokens if not token.isalpha() ]\n",
    "\n",
    "# Count the frequency of each token.\n",
    "# Answers the question, How many times does each token appear across the corpus?\n",
    "# Save the result to the 'freqs' variable\n",
    "freqs = FreqDist(notalpha)\n",
    "\n",
    "# Print the 100 most frequent tokens with non-alphabetical characters\n",
    "print( freqs.most_common(100) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, should we apply the filter to remove tokens with non-alphabetical characters?\n",
    "\n",
    "**It looks like there are a few kinds of tokens here**, all of which would be removed from our dataset if we filtered using `isalpha()`:\n",
    "<ul>\n",
    "    <li> Punctuation (i.e. commas, which appear 8484 times) </li>\n",
    "    <li> Numbers (i.e. '1.73', which appears twice) </li>\n",
    "    <li> Hypenated words (i.e. 'so-called', which appears 7 times) </li>\n",
    "</ul>\n",
    "\n",
    "For the purposes of this workshop, let's assume we want to remove all of these tokens from our data. But you might want to give a little bit of thought about how we can possibly filter the text before tokenizing it to save more tokens from unnecessary filtering.\n",
    "\n",
    ">**The next code chunk removes tokens that include non-alphabetical characters.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Remove tokens with characters that are not A-Z ##\n",
    "####################################################\n",
    "\n",
    "# Read the list of tokens and only return each token if it's all alphabetical\n",
    "# We write the result to a new variable, tokens_clean, which we will overwrite until it's cleaned.\n",
    "tokens_clean = [ token for token in tokens if token.isalpha() ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting text to lowercase\n",
    "\n",
    "**NLTK considers \"Amber\" and \"amber\" to be different tokens.** While it might be useful to make a distinction between these tokens (for example, for identifying some proper names), usually we choose to make all characters lowercase.\n",
    "\n",
    "We can convert all of the text into lowercase by passing each token using the `lower()` function. There are more nuanced ways to do this, such as by separating capitalization at the beginning of sentences versus within sentences, that might be more appropriate for your data. You will see an example of this selective transformat\n",
    "\n",
    ">**For our puposes, we will just convert all tokens to lowercase in the next code chunk.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "## Convert tokens to lowercase ##\n",
    "#################################\n",
    "\n",
    "# Use another list comprehension to save the lowercase version to tokens_clean (overwriting the original)\n",
    "tokens_clean = [ token.lower() for token in tokens_clean ]\n",
    "\n",
    "# Print the result\n",
    "print( tokens_clean )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency distributions\n",
    "\n",
    "The huge list of words above is impossible to interpret on its own. One simple way to analyze these data is to get the frequence (= count) of each token.\n",
    "\n",
    "### Zipf's Law\n",
    "\n",
    "The basic properties of the distribution of words in English was first described by Zipf (1932) in what would become known as Zipf's Law.\n",
    "\n",
    "While I will spare you the mathematical underpinnings, the gist is that only a handfull of words in English are relatively frequent while the vast majority are infrequent.\n",
    "\n",
    "If we did our job well, the plot below should support that conclusion.\n",
    "\n",
    "> The next code block calculates the frequency for every word in ``tokens_clean`` and plots the 50 most frequent.\n",
    "\n",
    "*Reference: Zipf, G. K., “Selected Studies of the Principle of Relative Frequency in Language,” Cambridge, MA: Harvard Univ. Press, 1932.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Generate a token frequency distribution ##\n",
    "#############################################\n",
    "\n",
    "# This line specifies the size of the figure as out Jupyter Notebook will print it\n",
    "plt.rcParams[\"figure.figsize\"] = [16,7]\n",
    "\n",
    "# Calculate the frequency for each token from the book and save it to the variable 'frequencies'\n",
    "frequencies = FreqDist( tokens_clean )\n",
    "\n",
    "# Call 'frequencies' with the method 'plot' to generate a frequency plot of the 50 most frequent words\n",
    "walden_plot = frequencies.plot( 50 )\n",
    "\n",
    "# The line above saves the plot to the variable 'walden_plot'. This line outputs it, so it will appear below.\n",
    "walden_plot\n",
    "\n",
    "# Also print the raw list\n",
    "frequencies.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare this plot to the empirical Zipf's Law distribution\n",
    "\n",
    "**The plot seems to confirm the hypothesis that the frequency distribution for Walden tokens follows Zipf's law.**\n",
    "\n",
    "Next we will generate an Zipf distribution using the `numpy` statistics package, so we can compare the shapes of the distributions more directly.\n",
    "\n",
    ">**The next code block generates and displays a plot of the distribution Zipf proposed for English word frequency. How similar is it to the frequency distribution for tokens in Walden?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "## Generate a Zipf distribution ##\n",
    "##################################\n",
    "\n",
    "# We will randomly draw 1000 samples from the Zipf distribution with the parameter 'a'\n",
    "a = 1.1\n",
    "s = np.random.zipf(a, 1000)\n",
    "\n",
    "# Truncate at x=20 and plot the density of the distribution\n",
    "# This is because most of the 'action' in this distribution is closer to zero\n",
    "count, bins, ignored = plt.hist(s[s<20], 20, histtype = 'step', fill = None, density=True)\n",
    "\n",
    "# This is how we 'bin' our random terms between 1 and 20, so we can count the frequency of spans over the number line.\n",
    "x = np.arange(1., 20.)\n",
    "\n",
    "# We want to compare the Zipf samples to the the zeta function, which is like the Zipf distribution.\n",
    "y = x**(-a) / special.zetac(a)\n",
    "\n",
    "# Plot our frequency distribution\n",
    "plt.plot(x, y/max(y), linewidth=2, color='r')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hopefully, they look similar to you too.\n",
    "\n",
    "## Removing stopwords\n",
    "\n",
    "The plot above shows we replicated Zipf's 1932 finding for Walden!\n",
    "\n",
    "One implication for this is that **most of the tokens in our dataset are not informative** because most of the high-frequency tokens in Walden are high frequency in every other English text. If we want to learn more about Walden, then we need to focus on lower frequency tokens.\n",
    "\n",
    "In other words, if we followed every step in this tutorial so far with a different text, the result would likely be exactly the same. If we are asking questions about English in general this might be useful, but instead of settling for this general dataset, we will remove the most common tokens to make our data more representative of Walden.\n",
    "\n",
    "Luckily, NLTK has many premade lists of very common tokens, and we already installed the lists! If you want to see it, just run the line `set(stopwords.words('english'))`.\n",
    "\n",
    ">**The next code block removes stopwords. This step looks a lot like filtering we did on non-alphabetical tokens.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "## Remove stopwords ##\n",
    "######################\n",
    "\n",
    "# Check every token in 'tokens_clean' against the NLTK stopword list\n",
    "# Only keep tokens NOT in the list\n",
    "tokens_clean = [t for t in tokens_clean if not t in set(stopwords.words('english'))]\n",
    "\n",
    "# Generate a new frequency distribution plot with the 50 most frequent words remaining after filtering\n",
    "# This follows the same steps as before: We count the frequency of each token, we generate a plot of the top 50, and then we display them.\n",
    "frequencies = FreqDist( tokens_clean )\n",
    "\n",
    "# Make plot with top 50 tokens\n",
    "walden_plot = frequencies.plot( 50 )\n",
    "\n",
    "# Show the plot\n",
    "walden_plot\n",
    "\n",
    "# Also print the raw list\n",
    "frequencies.most_common(50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning is time consuming and imperfect\n",
    "\n",
    "**As you can tell from the words in the plot above, we now have a fairly clean and informative data set!**\n",
    "\n",
    "For example, we can tell just by looking at this that Walden may have a lot to do with the natural world (water, pond, ice, winter, nature, world) and was perhaps about a man living in nature (man, men, time, house).\n",
    "\n",
    "Of course, there are other frequent tokens that might provide less information for your analysis (would, may, though, two, us). You can either filter these words out using *lists*, as we did above for non-alphabetical tokens and for stopwords, *or* you can filter our entire classes of words (i.e. pronouns) from your analysis. This will be detailed in the next section.\n",
    "\n",
    "First we will save our progress in the **pickle format**, which is an efficient way to store raw data in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "## Save our progress ##\n",
    "#######################\n",
    "\n",
    "# Write the tokens list to the /working/ folder\n",
    "with open('working/walden_clean_tokens.pkl', 'wb') as f:\n",
    "    pickle.dump( tokens_clean, f )\n",
    "\n",
    "# Also write a Text object (unique to NLTK) to the /working/ folder \n",
    "with open('working/walden_text.pkl', 'wb') as f:\n",
    "    pickle.dump( Text( tokens ), f )\n",
    "    \n",
    "# Also write the text to the /working/ folder \n",
    "with open('working/walden_raw.pkl', 'wb') as f:\n",
    "    pickle.dump( walden_raw, f )\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next: Text Analysis\n",
    "\n",
    "The next section will show the key features of NLTK's text analysis functions. We will compare the meanings of tokens, find similar tokens, analyze the sounds of Walden, and learn two ways to find the part of speech of each token (noun, verb, and so on)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code it: Tokenize Shakespeare\n",
    "\n",
    "**Adapt the code block below to tokenize the complete works of William Shakespeare.**\n",
    "\n",
    "* Below I give the filepath to a plaintext version of the complete works of William Shakespeare.\n",
    "* For the first part of this activity you will tokenize the text.\n",
    "* For the second part, you will generate a frequency distribution plot for the text.\n",
    "\n",
    "Keep in mind that it will take a little while to run these scripts because this text file is larger than Walden.\n",
    "*Tokenization takes about 10 seconds and cleaning + plotting takes about two minutes.*\n",
    "\n",
    "Answers are in /answer_keys/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################\n",
    "## ## ## ## > Code it < ## ## ## ##                              #\n",
    "################################### \"Cleaning_Shakespeare_Timed\" #\n",
    "## Sample answer in /answer_keys ##                              #\n",
    "##################################################################\n",
    "\n",
    "######################\n",
    "## Read-in the file ##\n",
    "######################\n",
    "\n",
    "# What is the path to the complete works of Shakespeare?\n",
    "shakespeare_path = 'texts/shakespeare.txt'\n",
    "\n",
    "# Open the file at the specified path\n",
    "shakespeare_file = open( shakespeare_path, 'r')\n",
    "\n",
    "# Read the file as raw text by calling the file with the read() function\n",
    "shakespeare_raw = \n",
    "\n",
    "# Close the original file with the close() function\n",
    "shakespeare_file.close()\n",
    "\n",
    "#######################\n",
    "## Tokenize the file ##\n",
    "#######################\n",
    "\n",
    "# Tokenize the raw text using word_tokenize()\n",
    "shakespeare_tokens = \n",
    "\n",
    "# Print a list of the tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code it, continued: Plot the token frequency distribution for Shakespeare\n",
    "\n",
    "**Adapt the code block below to plot the 50 most frequent tokens in the complete works of William Shakespeare.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############\n",
    "## Cleaning ##\n",
    "##############\n",
    "\n",
    "# Remove non-alphabetical characters using list comprehension\n",
    "shakespeare_tokens = [ token for token in shakespeare_tokens if token.isalpha() ]\n",
    "\n",
    "# Convert the text to lowercase using list comprehension\n",
    "shakespeare_tokens = [ token.lower() for token in shakespeare_tokens ]\n",
    "\n",
    "# Filter out stopwords using list comprehension\n",
    "shakespeare_tokens = [token for token in shakespeare_tokens if not token in set(stopwords.words('english'))]\n",
    "\n",
    "####################\n",
    "## Frequency plot ##\n",
    "####################\n",
    "# Generate a frequency distribution for the tokens\n",
    "\n",
    "\n",
    "# Display plot with top 50 tokens\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
