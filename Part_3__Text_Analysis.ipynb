{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Analysis\n",
    "\n",
    "**In this part we focus on tools that help you analyze the content of a text.**\n",
    "\n",
    "**Goals**\n",
    "<ol>\n",
    "    <li> Analyze words using a concordance </li>\n",
    "    <li> Create a dispersion plot </li>\n",
    "    <li> Compare tokens between texts </li>\n",
    "    <li> Map from text to sounds to look at syllable distributions </li>\n",
    "    <li> Stem the data to look at word meanings </li>\n",
    "    <li> Tag the tokens with their part of speech using a tagger and dictionary</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "\n",
    "## This new notebook is a new environment, so we have to set it up again.\n",
    "\n",
    "# Import packages\n",
    "from nltk import *\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import string\n",
    "from nltk.corpus import cmudict\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Import the Walden data we saved from the Text Cleaning chapter\n",
    "walden_tokens = pickle.load( open( \"working/walden_clean_tokens.pkl\", \"rb\" ) )\n",
    "walden_text = pickle.load( open( \"working/walden_text.pkl\", \"rb\" ) )\n",
    "walden_raw = pickle.load( open( \"working/walden_raw.pkl\", \"rb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concordances\n",
    "\n",
    "Concordances align one token across multple contexts so we can see at a glance how the word is being used. This can be very useful for understanding how two similar words might be used differently. We will use a concordance to compare \"wisdom\" and \"knowledge\" in the next code block to see how they work.\n",
    "\n",
    ">The next code block generates concordances for two words that share much of their meaning. If concordances can show differences in their meaning (by their context) then we will know more about how the words are used in Walden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "## Generate concordances ##\n",
    "###########################\n",
    "\n",
    "# This line uses the print() function to display text in the console (our notebook!)\n",
    "print(\"Concordance for Wisdom\") # Title\n",
    "\n",
    "# Concordance for \"wisdom\"\n",
    "walden_text.concordance(\"wisdom\")\n",
    "\n",
    "# Add a new line to separate the concordances\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"Concordance for Knowledge\") # Title\n",
    "\n",
    "# Note that > print(\"\\nConcordance for Knowledge\") < would give the exact same result\n",
    "\n",
    "# Concordance for \"water\"\n",
    "walden_text.concordance(\"knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing \"knowledge\" and \"wisdom\" in Walden**\n",
    "\n",
    "Maybe:\n",
    "* wisdom seems to be something that can be conveyed through text (recorded, in the Bible) and is a characteristic of humans (perhaps versus animals)\n",
    "* knowledge seems to be easier to qualify--masters have more of it but much happens \"without\" it\n",
    "\n",
    "\n",
    "## Positional similarity\n",
    "It can be difficult to compare concordances as they are displayed above, so it's common to look at positional similarity. This is helpful because similar words often appear in similar contexts. We will look at two functions that exploit this linguistic fact to find patterns in word use.\n",
    "<ul>\n",
    "<li>common_contexts() takes a set of tokens and returns the words that immediately precede and follow all words in the set.\n",
    "\n",
    "<li>similar() takes a token and returns other tokens that appear in the same context as itself.\n",
    "<ul>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################\n",
    "## Compare common token contexts and token similarity ##\n",
    "########################################################\n",
    "\n",
    "# Print common contexts for 'ice' and 'water'\n",
    "print(\"Common contexts for 'ice' and 'water'\") # Title\n",
    "walden_text.common_contexts([\"ice\", \"water\"])\n",
    "\n",
    "# Print similar tokens for 'water'\n",
    "print(\"\\nTerms similar to 'water'\") # Title\n",
    "walden_text.similar(\"water\")\n",
    "\n",
    "# Print similar tokens for 'ice'\n",
    "print(\"\\nTerms similar to 'ice'\") # Title\n",
    "walden_text.similar(\"ice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Comparing similarity of \"water\" and \"ice\" in Walden**\n",
    "\n",
    "Based on the *common contexts* it appears that:\n",
    "* both are nouns (follows 'the' and prepositions like 'in' and 'of')\n",
    "* both are at Walden (follows walden)\n",
    "* both are in the other's similar terms\n",
    "* water is similar to 'spring' -- which is when the pond will turn from ice to unfrozen water\n",
    "* ice is more similar to 'birds' -- maybe because ice restricts birds to the parts of the pond without it?\n",
    "\n",
    "We will look more at the way that the changing of the seasons may influence the text of Walden in the next part.\n",
    "\n",
    "## Dispersion plots\n",
    "\n",
    "Another way to \"eye\" the way tokens are used across a text is through a dispersion plot. These show where tokens appear in a text. For example, in a book such as Walden that records the seasons in chronological order, we would expect differences in how nature nouns appear by season.\n",
    "\n",
    "To test whether we can observe the changes of the seasons in Walden just by a text analysis, we will plot the dispersion of \"snow\" and \"heat\". If true, we should see little overlap between the offset (= position) of these words.\n",
    "\n",
    ">The following code block plots the location of two opposing weather terms in Walden. We hypothesize that they should not have overlapping offsets because \"heat\" does not cooccur with \"snow\" in nature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "## Generate a dispersion plot to compare two tokens ##\n",
    "######################################################\n",
    "\n",
    "# This line specifies the size of the figure as out Jupyter Notebook will print it\n",
    "plt.figure(figsize=(16,2))\n",
    "\n",
    "# Show a dispersion plot for the seasonal weather terms, \"snow\" and \"heat\"\n",
    "walden_text.dispersion_plot([\"snow\", \"heat\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **It looks like our hypothesis is correct -- you can see an alternation in seasonal weather terms in Walden.**\n",
    "\n",
    "Another way we can learn about how tokens are used in a text is look at what tokens often appear together, which is the topic of the next section.\n",
    "\n",
    "\n",
    "# Code it: Compare 'wisdom' and 'knowledge' according to Shakespeare\n",
    "\n",
    "**Modify the following code block to compare the tokens 'wisdom' and 'knowledge' in Shakespeare.**\n",
    "\n",
    ">As an extra challenge, use the loop to get the concordance and similar tokens for each token, then print the common contexts across all the tokens in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open a pre-tokenized (but not cleaned) version of the complete works of Shakespeare\n",
    "shakespeare_tokens = pickle.load( open( \"working/shakespeare_tokens.pkl\", \"rb\" ) )\n",
    "\n",
    "# Convert the tokens to a nltk Text object, which is how collocates are found\n",
    "shakespeare = Text( shakespeare_tokens )\n",
    "\n",
    "##################################################################\n",
    "## ## ## ## > Code it < ## ## ## ##                              #\n",
    "###################################  \"CodeIt_Shakepeare_Wisdom\"  #\n",
    "## Sample answer in /answer_keys ##                              #\n",
    "##################################################################\n",
    "\n",
    "# A list of words we want to compare\n",
    "words = []\n",
    "\n",
    "# Loop the list of words to compare\n",
    "for word in words:\n",
    "    continue\n",
    "    \n",
    "    # Concordance for the word\n",
    "\n",
    "    # Similar tokens for the word\n",
    "\n",
    "# Print common contexts for all words in the list (don't loop this)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collocations\n",
    "\n",
    "***As you have probably gathered, we can learn a lot about a word just by seeing it across many contexts.*** One special type of context has to do with words that frequently appear adjacent to one another. We say these tokens are collocated, meaning they appear together in the same location. Often these words are compound nouns, proper nouns, and idioms.\n",
    "\n",
    ">**The next code block shows the 100 most frequent collocated token pairs. In other words, adjacent tokens that appear together the most.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "## List frequently collocated tokens ##\n",
    "#######################################\n",
    "\n",
    "# Get the top 100 collocated token pairs for the text\n",
    "# Text() here transforms 'walden_tokens' into an NLTK text object, which is required for collocations()\n",
    "Text( walden_tokens ).collocations(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Let's try to list some of the types of collcated tokens we found:**\n",
    "\n",
    "Proper nouns\n",
    "* New England\n",
    "* John Field\n",
    "* Walden Pond\n",
    "\n",
    "Predicates\n",
    "* answer questions\n",
    "* mastered difficulties\n",
    "* hooting owl\n",
    "\n",
    "Idioms\n",
    "* take place\n",
    "* beaten track\n",
    "* good deal\n",
    "\n",
    "\n",
    "## Unique tokens\n",
    "\n",
    "So far, we have just looked at tokens within Walden and the complete works of Shakespeare. But how do they compare?\n",
    "\n",
    "There are many reasons to think they are different. For one thing, Walden was written over 250 years after Shakespeare's death -- before the first English novel was written! Next we will learn what tokens are unique between these texts.\n",
    "\n",
    ">**The next code block creates two lists of common tokens that are in one text but not the other.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################\n",
    "## List unique tokens in each text, relative to the other ##\n",
    "############################################################\n",
    "\n",
    "# Make sure both texts are lowercase\n",
    "walden_lower = [token.lower() for token in walden_tokens]\n",
    "shakespeare_lower = [token.lower() for token in shakespeare_tokens]\n",
    "\n",
    "# Get the frequency distributions for our tokenized texts\n",
    "walden_common = FreqDist( walden_lower )\n",
    "shakespeare_common = FreqDist( shakespeare_lower )\n",
    "\n",
    "# Get the 1000 most frequent tokens\n",
    "# Save the token only (index 0), leaving the frequency (index 1) behind\n",
    "walden_common = [token[0] for token in walden_common.most_common(1000)]\n",
    "shakespeare_common = [token[0] for token in shakespeare_common.most_common(1000)]\n",
    "\n",
    "# Convert our two lists of the 1000 most frequent tokens into sets, which removes duplicates\n",
    "walden_common = set( walden_common )\n",
    "shakespeare_common = set( shakespeare_common )              \n",
    "\n",
    "# Extract all tokens in Walden that do not appear in Shakespeare\n",
    "walden_special = walden_common.difference( shakespeare_common )\n",
    "\n",
    "# Extract all tokens in Shakespeare that do not appear in Walden\n",
    "shakespeare_special = shakespeare_common.difference( walden_common )\n",
    "\n",
    "# Print a pretty two-column list--ignore the details here\n",
    "print('%-8s%-20s%s' % ('', 'W NOT S', 'S NOT W'))\n",
    "for i, (wald, shak) in enumerate(zip(sorted(walden_special), sorted(shakespeare_special))):\n",
    "    print('%-8s%-20s%s' % (i, wald, shak))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping text to sounds\n",
    "\n",
    "**We can move our analysis from text to sound as long as we know how each word sounds.**\n",
    "\n",
    "There are many systems to represent human speech sounds orthographically (= written), the most prominent being the International Phonetic Alphabet (IPA, see https://en.wikipedia.org/wiki/International_Phonetic_Alphabet).\n",
    "\n",
    "* Problem: The IPA includes symbols that are not part of a normal keyboard.\n",
    "* Solution: Use the ARPABET version of the IPA, which is only uses standard characters.\n",
    "\n",
    ">**The following code block uses a pronunciation dictionary (CMUdict) to estimate what the spoken version of this text would sound like.**\n",
    "\n",
    "More on CMUdict here: http://www.speech.cs.cmu.edu/cgi-bin/cmudict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################\n",
    "## Look up a token in CMUdict ##\n",
    "################################\n",
    "\n",
    "# Create a shortcut to the CMUdict produnciation dictionary, 'cmu()'\n",
    "cmu = cmudict.dict()\n",
    "\n",
    "# Look at the entry for 'melting'\n",
    "print(cmu['melting'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to turn ```[['M', 'EH1', 'L', 'T', 'IH0', 'NG']]``` into an object of analysis\n",
    "\n",
    "**Not only does CMUdict give a breakdown of the sounds in many words, but it shows whether each vowel is stressed or not:**\n",
    "* 0 means No Stress\n",
    "* 1 means Primary Stress\n",
    "* 2 means Secondary Stress.\n",
    "\n",
    "In English, each syllable has a vowel at its core (= nucleus in phonology), so to get the number of syllables, we just have to count the number of vowels. Practically, that means we only need to count how many digits there are in the ARPABET transcription to get the number of syllables.\n",
    "\n",
    "**This means that according to the pronunciation dictionary, the word 'melting' has two syllables: EH and IH, which both have digits in their transcription.**\n",
    "\n",
    ">**In the next code block, we use a custom function to count syllables and combine that information with each token's frequency and the token itself.** While we will start out by making three lists as if they are three columns in a spreadsheet, and each element is a row, we will combine them into a `pandas dataframe`, which make anyalysis very convenient.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "## Create a dataset with the syllable count for common Walden tokens ##\n",
    "#######################################################################\n",
    "\n",
    "# Custom function to count the syllables for each word passed to it\n",
    "def syllables(word):\n",
    "    # Only look up the word if it's in CMUdict (set of 134,000 words)\n",
    "    if word in cmu:\n",
    "        # If it's in CMUdict, print the number of digits in the transcription\n",
    "        return max([len([y for y in x if y[-1] in string.digits])\n",
    "                    for x in cmu[word]])\n",
    "\n",
    "# Regenerate the frequency distribution across the tokens to extract the most common words\n",
    "token_frequencies = FreqDist( walden_tokens )\n",
    "\n",
    "# Query the frequency distribution to return the n most common words\n",
    "common_tokens = token_frequencies.most_common( 25 )\n",
    "\n",
    "# Extract words (as a list of strings) from the frequency distribution\n",
    "# Read this as \"for each thing in 'common', take the first item\".\n",
    "# Remember that in Python, we count from zero.\n",
    "common_words = [x[0] for x in common_tokens]\n",
    "\n",
    "# Their frequencies\n",
    "common_freq = [x[1] for x in common_tokens]\n",
    "\n",
    "# Their syllable count\n",
    "common_syll = [syllables(x[0]) for x in common_tokens]\n",
    "\n",
    "# Their rank, equal to the row number + 1\n",
    "common_rank = range( 1, (len(common_tokens) + 1) )\n",
    "\n",
    "# Format our data as a dictionary\n",
    "common_data = {'Rank':common_rank, 'Word':common_words, 'Freq':common_freq, 'Syllables':common_syll}\n",
    "\n",
    "# Convert that dictionary into a pandas dataframe (common data format)\n",
    "# &not a workshop on pandas... helping to visualize... workshops will be in the summer %\n",
    "common_data = pd.DataFrame.from_dict( common_data )\n",
    "\n",
    "# Display a pretty HTML table of our data\n",
    "display(HTML( common_data.to_html() ))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we do?\n",
    "\n",
    "**Most of the common tokens have one syllable, some have two syllables, and none have three or more.** Surely there are tokens in Walden with many syllables, but long words tend to be much less frequent.\n",
    "\n",
    "\n",
    "# Code it: What are the most frequent 3-syllable tokens in Walden?\n",
    "\n",
    "Modify the next code block to print the most frequent 3-syllable tokens in Walden.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################\n",
    "## ## ## ## > Code it < ## ## ## ##                      #\n",
    "################################### \"Walden_Trisyllables #\n",
    "## Sample answer in /answer_keys ##                      #\n",
    "##########################################################\n",
    "\n",
    "# Regenerate the frequency distribution for 'walden_tokens'\n",
    "#      Call it 'token_frequencies'\n",
    "token_frequencies = FreqDist( walden_tokens )\n",
    "\n",
    "# A placeholder for our trisyllable words\n",
    "trisyl_tokens = []\n",
    "\n",
    "# Loop every member of the frequency distribution (or use a list comprehension statement)\n",
    "for token in token_frequencies:\n",
    "    continue\n",
    "    \n",
    "    # Look up the token pronunciation and count the digits in CMUdict to get the number of syllables\n",
    "\n",
    "    # If the syllable count is equal to three...\n",
    "\n",
    "        # Add our token to the list\n",
    "        # walden_stems_sylls.append()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How did we do?\n",
    "\n",
    "It looks like we did pretty well! These are all words that *can* be pronounced with three syllables. Try it if you want.\n",
    "\n",
    "\n",
    "## Distribution of syllable counts\n",
    "\n",
    "Most of the common words in English have just one syllable, but we also found that some common words have two and many words that are less common have three. What does the distribution of syllables look like for Walden?\n",
    "\n",
    ">**In the next code chunk we generate a plot showing the distribution of syllable frequencies across the Walden tokens.** This is a useful step when we build programs to classify the reading difficulty of a text. Simpler texts use fewer polysyllabic words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Plot the distribution of syllable count ##\n",
    "#############################################\n",
    "\n",
    "# Get the syllable count for each token in Walden\n",
    "walden_syllables = [syllables(x) for x in walden_tokens]\n",
    "\n",
    "# Convert the list of counts to a pandas series for easy plotting\n",
    "walden_syllables = pd.Series(walden_syllables)\n",
    "\n",
    "# Set the figure size to fill the notebook\n",
    "plt.rcParams[\"figure.figsize\"] = [16,7]\n",
    "\n",
    "# Plot a histogram of the syllable counts\n",
    "walden_syllables.plot.hist(grid=True, rwidth=2,\n",
    "                   color='#607c8e')\n",
    "\n",
    "# Let's also add a title to our plot\n",
    "plt.title('Syllable Frequency in Walden')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back to the text domain\n",
    "\n",
    "Say we are comparing \"writes\" and \"writing\", both of which have present tense when used as a verb. 'Writes' has one syllable and 'writing' has two, but in many cases we do not really care about the difference between these words. Instead, we want to get statistics on the core meaning of the word.\n",
    "\n",
    "The next section is dedicated to reducing each token to its underlying word, called a *stem*.\n",
    "\n",
    "\n",
    "## Stemming\n",
    "\n",
    "**What do \"write\", \"writing\", \"written\", \"wrote\", \"writes\", \"writer\", \"rewrite\", \"rewrites\" all have in common?**\n",
    "\n",
    "They come from the same word, 'write', which we call the **stem** of the words in this list. We can programatically reduce each token to its stem using a process called *stemming*. Stemming the list above would result in each token being transformed into 'write'.\n",
    "\n",
    "> In the next code chunk, we stem each token in Walden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#################\n",
    "## Stem Walden ##\n",
    "#################\n",
    "\n",
    "# Stem all of the cleaned tokens\n",
    "walden_stems = [PorterStemmer().stem(t) for t in walden_tokens]\n",
    "\n",
    "# Regenerate the frequency distribution across the tokens to extract the most common words\n",
    "stem_frequencies = FreqDist( walden_stems )\n",
    "\n",
    "# Query the frequency distribution to return the n most common words\n",
    "common_stems = stem_frequencies.most_common( 20 )\n",
    "\n",
    "# What forms were removed? We take the differences between the tokens and stems and save them as a list to walden_diff.\n",
    "walden_diff = list(set(walden_tokens) - set(walden_stems))\n",
    "\n",
    "# Just for fun, calculate the percentage of tokens removed\n",
    "# We do this by dividing the number of tokens removed by the total, multiplying by 100, then rounding to the nearest integer using round().\n",
    "walden_diff_percent = round((len(walden_diff) / len(walden_tokens)) * 100)\n",
    "\n",
    "# How many word forms did stemming remove?\n",
    "print(\"We removed\", len(walden_diff), \"word forms (\", walden_diff_percent, \"% of the token count ).\")\n",
    "\n",
    "# What do the tokens look like before stemming?\n",
    "# Only show the first 50 tokens/stems\n",
    "print(\"\\nBefore stemming:\\n\", walden_diff[0:50])\n",
    "\n",
    "print(\"\\nAfter stemming:\\n\", walden_stems[0:50])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our 'primitive' stemmer did okay, but not great\n",
    "\n",
    "To review our results quickly:\n",
    "\n",
    "**Good stems**\n",
    "* wood\n",
    "* labor\n",
    "* hand\n",
    "\n",
    "**Bad stems**\n",
    "* inquiri (inquiries - es)\n",
    "* obtrud (I have no idea, do you?)\n",
    "* economi (economics - cs)\n",
    "\n",
    "If you are beginning to think you can build a better stemmer, you probably can! For our purposes, mispelled words are ignored, meaning we will have some data loss. Unlike many numerical tasks, text processing often requires custom solutions because of the inherent variability in language.\n",
    "\n",
    "> In the next code block, you will write some custom code to count all of the stems in <code>walden_stems</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code it: Count all syllables in the stemmed Walden\n",
    "\n",
    "Because stems do not include prefixes or suffixes, they tend to be phonologically shorter. We should be able to observe this in Walden by counting the syllables in the stems, which should show fewer polysyllabic words.\n",
    "\n",
    ">Test this hypothesis by modifying the next code block to create a list of syllable counts, i.e. <code>[1, 1, 2, 1, 3]</code>. Save this list to the variable <code>walden_stems_sylls</code>. You can do this using a loop or list comprehension. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################################\n",
    "## ## ## ## > Code it < ## ## ## ##                               #\n",
    "################################### \"CodeIt_Walden_Stem_Syllables #\n",
    "## Sample answer in /answer_keys ##                               #\n",
    "###################################################################\n",
    "\n",
    "##################\n",
    "## Loop version ##\n",
    "##################\n",
    "\n",
    "# Loops require an empty list\n",
    "# walden_stems_sylls = []\n",
    "\n",
    "# Loop each stem in walden_stems\n",
    "# for\n",
    "    \n",
    "    # Get the syllable count using our custom function, syllables()\n",
    "    # syll_count = \n",
    "    \n",
    "    # If the word is missing from CMUdict, it will return 'None'--filter those out\n",
    "    # if syll_count != None:\n",
    "    \n",
    "        # Add the syllable count to the list with append()\n",
    "        # walden_stems_sylls.append()\n",
    "\n",
    "################################\n",
    "## List comprehension version ##\n",
    "################################\n",
    "\n",
    "# Return the syllable count for each stem in walden stems, if it isn't missing from CMUdict\n",
    "# walden_stems_sylls = [x for x in y if x != None]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't edit this block--it's to plot the results of the Code It above!\n",
    "\n",
    "#############################################\n",
    "## Plot the distribution of syllable count ##\n",
    "#############################################\n",
    "\n",
    "# If 'walden_stems_sylls' exists, plot the syllable frequency distribution\n",
    "if 'walden_stems_sylls' in locals() or 'walden_stems_sylls' in globals():\n",
    "    \n",
    "    # Convert the list of counts to a pandas series for easy plotting\n",
    "    walden_stems_sylls = pd.Series( walden_stems_sylls )\n",
    "\n",
    "    # Set the figure size to fill the notebook\n",
    "    plt.rcParams[\"figure.figsize\"] = [16,7]\n",
    "\n",
    "    # Plot a histogram of the syllable counts\n",
    "    walden_stems_sylls.plot.hist(grid=True, rwidth=2, color='#607c8e')\n",
    "\n",
    "    # Let's also add a title to our plot\n",
    "    plt.title('Syllable Frequency in Walden Stems')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging part of speech\n",
    "\n",
    "**Another way to group tokens together is by part of speech (= lexical class).**\n",
    "\n",
    "To do this, we use the NLTK function `pos_tag()` which looks up each token in a dictionary and retreives the most common part of speech. The common parts of speech are noun (NN), verb (VB), adjective (JJ), and adverb (RB). There are many more types and subtypes that NLTK labels using this function, which you can learn about it by runnning the function `help.upenn_tagset()`.\n",
    "\n",
    ">In the following code block, we tag the part of speech of each stemmed token, convert it to a pandas series, and plot a histogram of the frequency of each type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "## Tag the tokens with their part of speech ##\n",
    "##############################################\n",
    "\n",
    "# Tag each token with its part of speech\n",
    "walden_tagged = pos_tag( walden_tokens )\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags = [x[1] for x in walden_tagged]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags = pd.Series(walden_tags)\n",
    "\n",
    "walden_tags.value_counts().plot(kind='bar').title.set_text('POS Distribution for Walden')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information extraction\n",
    "\n",
    "Say we want to find all of the proper names (NN in this part of speech tag set) for a text. You can imagine that you are building a script to automatically create a page index for a book. If you want to look up all of the pages that mention New York, for example, how would you do so?\n",
    "\n",
    "> In the next code chunk, we will see how well the part of speech tagger finds proper nouns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Find all of the tokens labeled proper down ##\n",
    "################################################\n",
    "\n",
    "# Loop all of the tagged tokens and print those where the tag is 'NPP' (= proper noun)\n",
    "\n",
    "# Go tag by tag by indexing them one at a time using a loop\n",
    "for i in range(0, len(walden_tagged)):\n",
    "    \n",
    "    # If the current word's ([i]) tag matches 'NNP', run the nested script\n",
    "    if walden_tagged[i][1] == 'NNP':\n",
    "        \n",
    "        # Print the current word\n",
    "        print( walden_tagged[i][0] )\n",
    "\n",
    "# Save our list of NNPs to compare it with another method later\n",
    "# This save each element in walden_tagged if its tag (position 1 in the list) is equal to 'NNP'\n",
    "nnp_old = [x for x in walden_tagged if x[1] == 'NNP']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Not so good, but why?**\n",
    "\n",
    "One way to tell whether a word is a proper noun in written English is capitalization, but we removed that information when we cleaned the text. Luckily for us, we can quickly create a new version of the data that is fairly clean and retains its capitalization.\n",
    "\n",
    "## Regex\n",
    "\n",
    "Regex is short for **regular expressions**, which is a way to write general pattern. For example, phone numbers are usually written in the same way: *an area code surrounded by parentheses, a space, three numbers, a hyphen, and four more numbers*. If we write this pattern as a regular expression, then we can programmatically extract every phone number formatted like this in a huge set of text.\n",
    "\n",
    "Have you ever wondered how spammers found your phone number, address, and emails? It's because these kind of data are easy to extract from text posted online or electronically available.\n",
    "\n",
    ">**In the next code chunk, I show two examples of how we can extract strings that match different regex patterns. The patterns are not as complicated as a phone number, but you can see the building blocks of this method.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "## Setup ##\n",
    "###########\n",
    "\n",
    "# Import regex package\n",
    "import re \n",
    "\n",
    "# Import the raw text of Walden, replacing all new lines with a space, to make it one line\n",
    "walden_raw_oneline = walden_raw.replace('\\n', ' ')\n",
    "\n",
    "####################\n",
    "## Regex examples ##\n",
    "####################\n",
    "\n",
    "## Extract every 4-digit number in Walden (likely years)\n",
    "\n",
    "# Pattern\n",
    "# -------> In regex, \"\\d\" matches any digit, so '\\d\\d\\d\\d' means a string of four digits\n",
    "four_digit = '\\d\\d\\d\\d'\n",
    "\n",
    "# Find all matches for the 'four_digit' pattern in 'walden_raw_oneline'\n",
    "four_digit_result = re.findall(four_digit, walden_raw_oneline) #print(result)\n",
    "\n",
    "# Print the list of matches as a set, to remove duplicates\n",
    "print( \"Four digits:\", set( four_digit_result ) )\n",
    "\n",
    "\n",
    "## Extract all capitalized words\n",
    "\n",
    "# Pattern\n",
    "# -------> In regex, \"[A-Z] matches all capitalized words and [a-z] all lowercase. '*' matches everthing!\n",
    "capitalized = '[A-Z][a-z]*'\n",
    "\n",
    "# Find all matches for the 'capitalized' pattern in 'walden_raw_oneline'\n",
    "capitalized_result = re.findall(capitalized, walden_raw_oneline)\n",
    "\n",
    "# Print the list of matches as a set, to remove duplicates\n",
    "print( \"\\nCapitalized:\", set( capitalized_result ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it's time to use regex to help us do part of speech tagging!\n",
    "\n",
    ">**In the next code block, we create another version of the data that preserves (informative) capitalization. That means we will transform the first letter of every sentence into lowercase, preserving non-initial capitalization.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################\n",
    "## Make sentence-initial characters lowercase ##\n",
    "################################################\n",
    "\n",
    "# Copy the one-line version of Walden into the variable 'walden_raw_caps'\n",
    "walden_raw_caps = walden_raw_oneline\n",
    "\n",
    "# We prepare regex for the following rule\n",
    "#    '[.!?]' looks for expressions that begin with these punctuation marks\n",
    "#    '\\ +'   looks for one or more spaces.\n",
    "#                 The '\\' symbol \"escapes\" the space behind it...\n",
    "#                      ...telling regex to treat the space as a character to search for.\n",
    "#    '[A-Z]' looks for any capitalized character\n",
    "p = re.compile( \"[.!?]\\ +[A-Z]\" )\n",
    "\n",
    "# Loop every regex match in 'walden_raw_oneline'\n",
    "for match in p.finditer( walden_raw_oneline ):\n",
    "    \n",
    "    # Replace each match in 'walden_raw_caps' with its lowercase version as soon as we find it\n",
    "    walden_raw_caps = walden_raw_caps.replace( match.group(), match.group().lower() )\n",
    "\n",
    "# Print the original raw text (on one line) and the version...\n",
    "#      ...with sentence initial lowercase characters (walden_raw_caps)\n",
    "print( \"^Original:\", walden_raw.replace('\\n', ' ')[:1000] )\n",
    "print( \"\\n^Replacement:\", walden_raw_caps[:1000] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Now that the first letter of every sentence had been made lowercase, capitalization should be a better indication that a token is a proper noun.**\n",
    "\n",
    "> In the next code bock, we repeat the steps we took before with the all-lowercase text, from cleaning to tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################\n",
    "## Tokenize, clean, and tag Walden from scratch ##\n",
    "##################################################\n",
    "\n",
    "# Tokenize the raw text\n",
    "walden_tokens_caps = word_tokenize( walden_raw_caps )\n",
    "\n",
    "# Remove nonalphabetical tokens\n",
    "walden_tokens_caps = [ token for token in walden_tokens_caps if token.isalpha() ]\n",
    "\n",
    "# Remove stopword tokens\n",
    "walden_tokens_caps = [token for token in walden_tokens_caps if not token in set(stopwords.words('english'))]\n",
    "\n",
    "# We will NOT change capitalization, to see whether that changes our named entity list\n",
    "\n",
    "# Tag each token with its part of speech\n",
    "walden_tagged_caps = pos_tag( walden_tokens_caps )\n",
    "\n",
    "# Print the list of tokens and their tag\n",
    "walden_tagged_caps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "## Print every token tagged as a proper noun ##\n",
    "###############################################\n",
    "\n",
    "# Exactly as before, we loop every tag and print the word when its tag matches \"NNP\"\n",
    "for i in range(0, len(walden_tagged_caps)):\n",
    "    if walden_tagged_caps[i][1] == 'NNP':\n",
    "        print( walden_tagged_caps[i] )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################\n",
    "## Plot the distribution of parts of speech again ##\n",
    "####################################################\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags_caps = [x[1] for x in walden_tagged_caps]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags_caps = pd.Series( walden_tags_caps )\n",
    "\n",
    "# Create a bar plot of the counts of each tag (= histogram)\n",
    "walden_tags_caps_plot_updated = walden_tags_caps.value_counts().plot('bar')\n",
    "\n",
    "# Let's also add a title to our plot\n",
    "plt.title('POS Frequency in Walden (updated)')\n",
    "\n",
    "# Show the plot\n",
    "walden_tags_caps_plot_updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Looks good! There are a lot of proper nouns, *but* also a lot of capitalized words. It looks like this part of speech tagger does not do too well in identifying proper nouns.**\n",
    "\n",
    "Next we will look at a very different way to look up the part of speech of a word.\n",
    "\n",
    "## WordNet\n",
    "\n",
    "NLTK's default part of speech tagger uses capitalization to guess proper nouns, but there \n",
    "\n",
    "\n",
    "\n",
    "Instead of using capitalization and context to try to label each part of speech, let's look each word up in a dictionary. WordNet is a kind of dictionary that comes with NLTK that helps look up synonymns, definitions, and parts of speech (one per definition).\n",
    "\n",
    "> In the next code chunk, we create a list of all of the unique words in Walden and look them up one by one, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################################################\n",
    "## Tag each token with its most frequent 'dictionary' part of speech ##\n",
    "#######################################################################\n",
    "\n",
    "# Import WordNet from NLTK\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Collapse the cleaned tokens into a set, which means every element will be unique\n",
    "walden_tokens_unique = set( walden_tokens )\n",
    "\n",
    "# Empty list which will hold each unique token and its part of speech\n",
    "walden_tagged_wordnet = []\n",
    "\n",
    "# Loop over every unique token\n",
    "for token in walden_tokens_unique:\n",
    "    \n",
    "    # Look up the token in WordNet and extract its part of speech, using the pos() function\n",
    "    # Returns a list of parts of speech, one for every definition\n",
    "    token_pos = [ss.pos() for ss in wordnet.synsets( token )]\n",
    "    \n",
    "    # If we cannot find any definitions for the token\n",
    "    # % why greater than 1?\n",
    "    if len(token_pos) > 0:\n",
    "\n",
    "        # We take only the part of speech that occurs the most\n",
    "        token_pos = max(set( token_pos ), key = token_pos.count)\n",
    "        \n",
    "        # We count the occurances of the token in walden_tokens (a list)\n",
    "        token_freq = walden_tokens.count(token)\n",
    "\n",
    "        # Update the dictionary list with a list of information for this token\n",
    "        # A list of lists!\n",
    "        walden_tagged_wordnet.append( [token, token_pos, token_freq] )\n",
    "        \n",
    "        # Print the token, its part of speech, and its frequency as we go\n",
    "        print( token, token_pos, token_freq )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It looks like the part of speech tagger did a great job, even without capitalization.\n",
    "\n",
    "**Next we will compare these results to the default nltk part of speech tagger.**\n",
    "\n",
    ">The next block shows the part of speech frequency distribution from both tagging methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################################################\n",
    "## Plot the frequency distribution of the parts of speech for both methods ##\n",
    "#############################################################################\n",
    "\n",
    "# Take only the tags (word is index 0, part of speech tag is index 1)\n",
    "walden_tags_wordnet = [x[1] for x in walden_tagged_wordnet]\n",
    "\n",
    "# Convert the list of POS tags to a pandas series for easy plotting\n",
    "walden_tags_wordnet = pd.Series( walden_tags_wordnet )\n",
    "\n",
    "# Create a blank figure and axes to hold our subplots\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2)\n",
    "\n",
    "# Subplot 1 (left)\n",
    "walden_tags.value_counts().plot(ax=axes[0], kind='bar').title.set_text('NLTK default (Porter)')\n",
    "\n",
    "# Subplot 2 (right)\n",
    "walden_tags_wordnet.value_counts().plot(ax=axes[1], kind='bar').title.set_text('NLTK WordNet')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We can see there are differences between both part of speech tagging methods.\n",
    "\n",
    "How would the graph change if we used `walden_stems` instead of `walden_tokens`? Change the code to evaluate your hypothesis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code it: Longest modifier in Shakespeare\n",
    "\n",
    "In the following code block, write a script that returns the longest modifier in the complete works of William Shakespeare.\n",
    "\n",
    "* Modifiers are adjectives and adverbs, tagged \"JJ\" and \"RB\" respectively. This is the tag set used by the NLTK function pos_tag().\n",
    "\n",
    "* There are many ways to do this one, but only one word that meets the prompt!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################################\n",
    "## ## ## ## > Code it < ## ## ## ##                           #\n",
    "################################### \"CodeIt_Longest_Modifier\" #\n",
    "## Sample answer in /answer_keys ##                           #\n",
    "###############################################################\n",
    "\n",
    "# First we have to tag each token with its part of speech\n",
    "shakespeare_tagged = pos_tag( shakespeare_tokens )\n",
    "\n",
    "# Your code here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final challenge: Quantify reading difficulty level\n",
    "\n",
    "**If you have some extra time, try writing a script that estimates how difficult a text is to read.**\n",
    "\n",
    "Here are some factors to consider:\n",
    "* **Text length** (longer text overall -> overall)\n",
    "* **Distribution of token frequency** (more low frequency tokens -> more difficult)\n",
    "* **Distribution of syllable frequency** (longer to pronounce -> more difficult)\n",
    "* **Distribution of part of speech frequency** (more modifiers --> more difficult)\n",
    "* **Characters per word** (different than syllable count, but possibly informative)\n",
    "* **Words per sentence** (consider using regex here)\n",
    "* **Number of similar tokens** (more similar tokens overall suggests more robust writing)\n",
    "\n",
    "Your script should take the raw text file (.txt) as input and should return a number between 0 and 10, 0 for very simple texts and 10 for very difficult texts. While I have lots of ideas about how to write this kind of script, I did not write any of them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are shorcuts to the raw texts of Shakespeare and Walden, as pickles\n",
    "shakespeare_raw = pickle.load( open( \"working/shakespeare_raw.pkl\", \"rb\" ) )\n",
    "walden_raw      = pickle.load( open( \"working/walden_raw.pkl\", \"rb\" ) )\n",
    "\n",
    "# Let's make each text one line by replacing all the newline symboles (\\n), tabs (\\t) and extra spaces with a singlespace\n",
    "walden      = re.sub('[ \\t\\n]+', ' ', walden_raw)\n",
    "shakespeare = re.sub('[ \\t\\n]+', ' ', shakespeare_raw)\n",
    "\n",
    "# Check our data\n",
    "print(walden[:100], \"\\n\\n\", shakespeare[:100])\n",
    "\n",
    "# Your code here:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
